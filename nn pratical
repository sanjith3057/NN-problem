# Program 01: Write a Python program to draw a simple graph using Matplotlib

from matplotlib import pyplot as plt

X=[1,2,3]
Y=[10,7,15]
plt.plot(X,Y)
plt.title("graph")
plt.xlabel("x axis")
plt.ylabel("y axis")
plt.show()     

#Program 02: Implement the Perceptron using AND function

#And function
import numpy as np
def unitstep(v):
  if v >= 0:
     return 1
  else:
     return 0

def perceptronmodel(x, w, b):
  v = np.dot(w, x) + b
  y = unitstep(v)
  return y
# w1 = 1, w2 = 1, b = -1
def ANDfunction(x):
  w = np.array([1, 1])
  b = -1.5
  return perceptronmodel(x, w, b)

test1 = np.array([0, 1])
test2 = np.array([1, 1])
test3 = np.array([0, 0])
test4 = np.array([1, 0])
print("AND({}, {}) = {}".format(0, 1, ANDfunction(test1)))
print("AND({}, {}) = {}".format(1, 1, ANDfunction(test2)))
print("AND({}, {}) = {}".format(0, 0, ANDfunction(test3)))
print("AND({}, {}) = {}".format(1, 0, ANDfunction(test4)))

#program 03: Program to calculate weights and bias in neural networks

import numpy as np

input = np.array([0.5,1])
weights = np.array([1,0.5])
bias = 0.8

input*weights
np.sum(input*weights)+bias

#program 04: To visualize the step function graph in neural networks
import numpy as np
import matplotlib.pyplot as plt

def step_function(x):
    return np.where(x < 0, 0, 1)

X = np.linspace(-10, 10, 400)
Y = step_function(X)

plt.plot(X,Y, color='green',label='step function')
plt.grid()
plt.xlabel('x')
plt.ylabel('y')
plt.title('step function')
plt.legend()
plt.show()

#program 05: Program to perform the sigmoid function

import numpy as np
import matplotlib.pyplot as plt

X = np.arange(-10, 10, 1)
Y = 1 / (1 + np.exp(-X))

plt.plot(X,Y, label='sigmoid')
plt.grid(True)
plt.legend()
plt.show

#program 06: Write a python program for matrix multiplication in neural networks

import numpy as np

def matrix_manipulation(A, B):
    if A.shape[1]!= B.shape[0]:
        raise ValueError("The number of columns in A must be equal to the number of rows in B")

    result = np.dot(A, B)
    return result

if __name__ == '__main__':
    A = np.array([[4, 2], [2, 4]])
    B = np.array([[1, 2], [3, 4]])

    print("Matrix A:")
    print(A)
    print("Matrix B:")
    print(B)
    print("Result:")
    print(matrix_manipulation(A, B))

#program 07: Calculate the partial derivative, for x1 when x0=3 and x1=4

import sympy as sp

# def the variable
X0 , X1 = sp.symbols('X0,X1')
f = X0**2 + X1**2
partial_X1 = sp.diff(f, X1)
value = partial_X1.subs({X0: 3, X1: 4})
# print the result
print(f"Partial derivative of f with respect to x1: {value}")

#program 08: Implement the affine and softmax layers

import numpy as np
x=np.array([[1,2,3,4]])
weights=np.random.randn(4,3)
bias=np.zeros((3,))
affine_out=np.dot(x,weights)+bias
print("output:",affine_out)
softmax_out = np.exp(affine_out) / np.sum(np.exp(affine_out), axis=1, keepdims=True)
print("Output:", softmax_out)

# ADVANCE PROGRAM

#program 09: Write a python program to perform various Activations functions

import numpy as np
import matplotlib.pyplot as plt

# Define the functions
step_function = lambda x: np.where(x < 0, 0, 1)
sigmoid = lambda x: 1 / (1 + np.exp(-x))
relu = lambda x: np.maximum(0, x)
tanh = lambda x: np.tanh(x)

# Generate data
X = np.linspace(-10, 10, 400)
Y_step = step_function(X)
Y_sigmoid = sigmoid(X)
Y_relu = relu(X)
Y_tanh = tanh(X)

# Plot the functions
plt.figure(figsize=(12, 8))

# Step Function
plt.subplot(2, 2, 1)
plt.plot(X, Y_step, color='green', label='Step Function')
plt.grid(True)
plt.xlabel('x')
plt.ylabel('y')
plt.title('Step Function')
plt.legend()

# Sigmoid Function
plt.subplot(2, 2, 2)
plt.plot(X, Y_sigmoid, color='blue', label='Sigmoid Function')
plt.grid(True)
plt.xlabel('x')
plt.ylabel('y')
plt.title('Sigmoid Function')
plt.legend()

# ReLU Function
plt.subplot(2, 2, 3)
plt.plot(X, Y_relu, color='red', label='ReLU Function')
plt.grid(True)
plt.xlabel('x')
plt.ylabel('y')
plt.title('ReLU Function')
plt.legend()

# Tanh Function
plt.subplot(2, 2, 4)
plt.plot(X, Y_tanh, color='orange', label='Tanh Function')
plt.grid(True)
plt.xlabel('x')
plt.ylabel('y')
plt.title('Tanh Function')
plt.legend()

plt.tight_layout()
plt.show()

#program 10: Develop a python program to visualize the distribution of hidden layers of activation function

import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

input_data = np.random.randn(1000, 100)
node_num = 100
hidden_layer_size = 5
activations = {}
x = input_data

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i-1]
    w = np.random.randn(node_num, node_num)
    a = np.dot(x, w)
    z = sigmoid(a)
    activations[i] = z

for i, a in activations.items():
    plt.subplot(1, len(activations), i+1)
    plt.title(f"{i+1}-layer")
    if i != 0:
        plt.yticks([], [])
    plt.hist(a.flatten(), 30, range=(0,1))

plt.show()

#program 11: Build the convolution layer in CNN

import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import convolve2d

# Input data and filter
input_data = np.array([[1, 2, 3, 0, 1],
                       [0, 1, 2, 3, 0],
                       [1, 0, 1, 2, 3],
                       [3, 1, 0, 1, 2],
                       [2, 3, 1, 0, 1]])
filter = np.array([[1, 0, -1],
                   [1, 0, -1],
                   [1, 0, -1]])

# Apply convolution
output = convolve2d(input_data, filter, mode='valid')

# Visualize input and output
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.title("Input Image")
plt.imshow(input_data, cmap='gray')
plt.colorbar()

plt.subplot(1, 2, 2)
plt.title("Convolved Feature Map")
plt.imshow(output, cmap='gray')
plt.colorbar()

plt.show()

#program 12:  Implement ReLU Function in Neural networks

import numpy as np
import matplotlib.pyplot as plt

def relu(x):
  return np.maximum(0,x)

x= np.linspace(-10,10,400)
Y=relu(x)

plt.plot(x,Y,label='relu_function')
plt.grid()
plt.legend()
plt.show()


#program 13: To Perform Image classification using Neural NetworkÂ algorithms
import tensorflow as tf
import matplotlib.pyplot as plt

# Load and preprocess the MNIST dataset
(x, y), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x, x_test = x / 255, x_test / 255

# Build and compile the model
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train and evaluate the model
model.fit(x, y, epochs=3)
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f'Test Loss: {test_loss}, Test Accuracy: {test_acc}')

# Predict and display the first test image and its predicted label
plt.imshow(x_test[0], 'binary')
plt.title(f'Predicted Label: {model.predict(x_test[:1]).argmax()}')
plt.show()
